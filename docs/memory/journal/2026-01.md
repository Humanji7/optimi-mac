# Error Journal: January 2026

---

## ERR-001: Literal Command in ENV Variable

**Date:** 2026-01-14
**Project:** Point-G
**Agent:** Claude Code / Human

**Что случилось:**
Railway application hung during boot. Dashboard showed `SECRET_KEY_BASE` was "set" but app couldn't decrypt credentials.

**Почему:**
- [x] Human oversight

The variable was literally set to `"openssl rand -hex 64"` instead of the actual hex result. Copy-pasting instructions without executing them.

**Урок:**
Always verify ENV variables with `railway variables --kv` — UI truncates values and can hide command strings.

**Prevention:**
Add to deployment checklist: "Verify all ENV values are results, not commands"

**Tags:** #railway #deployment #env

---

## ERR-002: Solid Queue Boot Deadlock

**Date:** 2026-01-14
**Project:** Point-G
**Agent:** Rails 8 / Puma

**Что случилось:**
App crashed on boot with truncated logs. The real error was buried: `solid_queue_recurring_tasks` table doesn't exist.

**Почему:**
- [x] Agent limitation (framework design)

Solid Queue runs as Puma plugin. If DB schema is missing, Puma crashes. But you can't run migrations because Rails loads the env which triggers the plugin which hits the missing table.

**Урок:**
Use `bin/rails db:prepare` not `db:migrate` after fresh deploys. Or comment out worker plugin temporarily.

**Prevention:**
Add to Rails 8 deployment checklist: "Verify solid_queue tables exist before enabling Puma plugin"

**Tags:** #rails8 #solid-queue #boot

---

## ERR-003: Volume Mount Path Mismatch

**Date:** 2026-01-14
**Project:** Point-G
**Agent:** Railway Templates

**Что случилось:**
Postgres service marked FAILED. Error: "requires volume at /var/lib/postgresql/data but mounted at /rails/storage"

**Почему:**
- [x] Lack of context

Railway template auto-configured a shared volume name, but each service needs its own mount path.

**Урок:**
Always verify Volume mount paths per service:
- Postgres: `/var/lib/postgresql/data`
- Rails: `/rails/storage`

**Prevention:**
Check Dashboard → Service → Settings → Volumes after any template deployment

**Tags:** #railway #volume #postgres

---

## ERR-004: KLYAP Watermark Removal Failed

**Date:** 2026-01-14
**Project:** Personal Site (KLYAP)
**Agent:** Antigravity

**Что случилось:**
Script processed images but Gemini watermarks remained visible. Black pixels weren't being removed.

**Почему:**
- [x] Agent limitation

The cropping logic assumed watermark was always in bottom 10% of image. But Gemini's watermark positioning varied.

**Урок:**
AI-generated images have unpredictable watermark positions. Need adaptive crop detection, not fixed percentages.

**Prevention:**
Use visual inspection after batch processing. Add "validate no watermarks" step to asset pipeline.

**Tags:** #image-processing #gemini #klyap

---

## ERR-005: Active Storage Orphaned Blobs (404s)

**Date:** 2026-01-14
**Project:** Point-G
**Agent:** Claude Code

**Что случилось:**
Production site returned 404 for some images. Database had blob records pointing to non-existent files.

**Почему:**
- [x] Lack of context

4 blob records were created with `service_name: local` before Cloudinary was configured. Railway's ephemeral disk wiped them.

**Урок:**
Check `ActiveStorage::Blob.where(service_name: :local)` before switching storage services. Orphaned records cause silent 404s.

**Prevention:**
Add migration to update orphaned blob `service_name` when switching storage services.

**Tags:** #rails #active-storage #railway #404

---

*Entries: 5 | Updated: 2026-01-15*

## ERR-006: Test Entry

**Date:** 2026-01-15
**Project:** optimi-mac
**Agent:** Antigravity

**Что случилось:**
This is a test entry to verify the log-error script works correctly.

**Почему:**
Testing the Living Memory system.

**Урок:**
Test successful

**Tags:** #memory #test #script

---

## ERR-007: Context Overflow Test

**Date:** 2026-01-16
**Project:** optimi-mac
**Agent:** Antigravity
**Type:** Context overflow

**Что случилось:**
Агент в середине длинной сессии забыл о ранее обсуждённых настройках и начал спрашивать заново про конфигурацию, которую уже настроили час назад.

**Почему:**
Контекст сессии превысил ~100k токенов. LLM начал "терять" ранний контекст из sliding window.

**Урок:**
При длинных сессиях делать summary каждые 30-40 минут или использовать HANDOFF документы для сохранения критичного контекста.

**Tags:** #context #llm #memory #taxonomy-test

---
